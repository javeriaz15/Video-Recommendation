{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyM/aPBiKZNgD/i1RndJQJ6K"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWFyJwRYErjh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# URL of the raw dataset file on GitHub\n",
        "url = \"https://raw.githubusercontent.com/javeriaz15/Video-Recommendation/refs/heads/main/dataset/RS_Fakedata-7-35_users.json\"\n",
        "\n",
        "# Load the data into a DataFrame\n",
        "data = pd.read_json(url)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "Cp1IzX_RM0Lb",
        "outputId": "87029065-a9d5-4d7e-885f-a4314c85d30e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   user_id country         city state  age  \\\n",
            "0        1     USA  Los Angeles    CA   30   \n",
            "1        2     USA     New York    NY   30   \n",
            "2        3     USA  Los Angeles    CA   18   \n",
            "3        4  Canada      Toronto    ON   40   \n",
            "4        5  Canada    Vancouver    BC   18   \n",
            "\n",
            "                                    video_link      genre  watched  liked  \\\n",
            "0  https://www.youtube.com/watch?v=D4vN_5MBEog    hip hop      0.3  False   \n",
            "1  https://www.youtube.com/watch?v=7iqMNnzQPmY     ballet      0.5  False   \n",
            "2  https://www.youtube.com/watch?v=D4vN_5MBEog    hip hop      0.3   True   \n",
            "3  https://www.youtube.com/watch?v=p0VGHuaICyI  classical      0.1  False   \n",
            "4   https://www.youtube.com/shorts/fv5vCREiBMQ      k pop      0.1  False   \n",
            "\n",
            "   skipped  \n",
            "0     True  \n",
            "1     True  \n",
            "2     True  \n",
            "3     True  \n",
            "4     True  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# URL of the video catalog file on GitHub\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/javeriaz15/Video-Recommendation/refs/heads/main/dataset/Video_catalog.json\"\n",
        "video_catalog = pd.read_json(url)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(video_catalog.head())\n"
      ],
      "metadata": {
        "id": "TJlcR93ANAuu",
        "outputId": "98b6fcbc-f3b8-4614-fc60-945ee2b785b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   video_id                                   video_link      genre  country  \\\n",
            "0         1  https://www.youtube.com/watch?v=D4vN_5MBEog    hip hop      USA   \n",
            "1         2  https://www.youtube.com/watch?v=7iqMNnzQPmY     ballet      USA   \n",
            "2         3  https://www.youtube.com/watch?v=p0VGHuaICyI  classical   Canada   \n",
            "3         4   https://www.youtube.com/shorts/fv5vCREiBMQ      k pop   Canada   \n",
            "4         5   https://www.youtube.com/shorts/kF0MRowRcIM    African  Nigeria   \n",
            "\n",
            "          city age_group  \n",
            "0  Los Angeles     18-35  \n",
            "1     New York     18-35  \n",
            "2      Toronto     35-50  \n",
            "3    Vancouver     18-25  \n",
            "4         Kano     35-50  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from surprise import Dataset, Reader, SVD, accuracy\n",
        "from surprise.model_selection import train_test_split\n",
        "from lightfm import LightFM\n",
        "from lightfm.data import Dataset as LightFMDataset\n",
        "from lightfm.evaluation import precision_at_k\n",
        "import numpy as np\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "# Configure logging for errors only\n",
        "logging.basicConfig(filename=\"recommendation_system.log\", level=logging.ERROR)\n",
        "\n",
        "# Define URL parameters for flexibility\n",
        "USER_DATA_URL = \"https://raw.githubusercontent.com/javeriaz15/Video-Recommendation/refs/heads/main/dataset/RS_Fakedata-7-35_users.json\"\n",
        "VIDEO_CATALOG_URL = \"https://raw.githubusercontent.com/javeriaz15/Video-Recommendation/main/dataset/Video_catalog.json\"\n",
        "\n",
        "# Load data with error handling and retry mechanism\n",
        "def load_data(url):\n",
        "    try:\n",
        "        return pd.read_json(url)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading data from {url}: {e}\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if loading fails\n",
        "\n",
        "user_data = load_data(USER_DATA_URL)\n",
        "video_catalog = load_data(VIDEO_CATALOG_URL)\n",
        "\n",
        "# Recommendation model weights\n",
        "SVD_WEIGHT = 0.6\n",
        "LIGHTFM_WEIGHT = 0.4\n",
        "\n",
        "# Collaborative filtering with Surprise SVD model\n",
        "reader = Reader(rating_scale=(0, 1))\n",
        "surprise_data = Dataset.load_from_df(user_data[['user_id', 'video_link', 'liked']], reader)\n",
        "trainset, testset = train_test_split(surprise_data, test_size=0.2)\n",
        "svd_model = SVD()\n",
        "svd_model.fit(trainset)\n",
        "\n",
        "# Predict and calculate RMSE\n",
        "predictions = svd_model.test(testset)\n",
        "rmse = accuracy.rmse(predictions)\n",
        "logging.info(f\"SVD Model RMSE: {rmse}\")\n",
        "\n",
        "# Encode video features for LightFM model\n",
        "video_catalog['video_id'] = video_catalog['video_link'].factorize()[0]\n",
        "user_data['user_id'] = user_data['user_id'].astype(str)\n",
        "user_data['video_id'] = user_data['video_link'].map(video_catalog.set_index('video_link')['video_id'])\n",
        "\n",
        "# Initialize LightFM dataset and build combined item features\n",
        "lfm_dataset = LightFMDataset()\n",
        "lfm_dataset.fit(users=(x for x in user_data['user_id'].unique()),\n",
        "                items=(x for x in video_catalog['video_id'].unique()),\n",
        "                item_features=(x for x in video_catalog['genre']))\n",
        "\n",
        "# Extract features with optimized apply function\n",
        "def extract_item_features(df, feature_cols):\n",
        "    return list(zip(df['video_id'], df[feature_cols].apply(lambda x: list(map(str, x)), axis=1)))\n",
        "\n",
        "feature_columns = ['genre', 'age', 'city', 'state']\n",
        "item_features = lfm_dataset.build_item_features(extract_item_features(video_catalog, feature_columns))\n",
        "\n",
        "# LightFM model training with reduced epochs\n",
        "lfm_model = LightFM(loss='warp')\n",
        "(interactions, weights) = lfm_dataset.build_interactions(\n",
        "    ((str(row['user_id']), row['video_id']) for _, row in user_data.iterrows())\n",
        ")\n",
        "lfm_model.fit(interactions, item_features=item_features, epochs=15, num_threads=4)\n",
        "\n",
        "# Precision@5 for LightFM\n",
        "precision = precision_at_k(lfm_model, interactions, item_features=item_features, k=5).mean()\n",
        "logging.info(f\"LightFM Precision@5: {precision}\")\n",
        "\n",
        "# Time decay function to adjust ratings\n",
        "def time_decay(timestamp, decay_rate=0.001):\n",
        "    days_ago = (datetime.now() - datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%S\")).days\n",
        "    return np.exp(-decay_rate * days_ago)\n",
        "\n",
        "# Apply time decay to liked ratings\n",
        "user_data['decayed_liked'] = user_data.apply(\n",
        "    lambda row: row['liked'] * time_decay(row['timestamp']) if row['liked'] else 0, axis=1\n",
        ")\n",
        "\n",
        "# Calculate engagement thresholds\n",
        "MIN_INTERACTIONS_FOR_ACTIVE = int(user_data.groupby('user_id').size().quantile(0.5))\n",
        "HIGH_WATCH_THRESHOLD = user_data['watched'].quantile(0.75)\n",
        "LOW_WATCH_THRESHOLD = user_data['watched'].quantile(0.25)\n",
        "\n",
        "# Engagement classifier\n",
        "user_interactions_cache = user_data.groupby('user_id').apply(lambda x: {\n",
        "    'total_interactions': len(x),\n",
        "    'avg_watch': x['watched'].mean(),\n",
        "    'total_skipped': x['skipped'].sum()\n",
        "}).to_dict()\n",
        "\n",
        "def classify_user_engagement(user_id):\n",
        "    user_metrics = user_interactions_cache.get(user_id, {})\n",
        "    total_interactions = user_metrics.get('total_interactions', 0)\n",
        "    avg_watch = user_metrics.get('avg_watch', 0)\n",
        "    total_skipped = user_metrics.get('total_skipped', 0)\n",
        "\n",
        "    if total_interactions < MIN_INTERACTIONS_FOR_ACTIVE:\n",
        "        return \"new_user\"\n",
        "    elif avg_watch < LOW_WATCH_THRESHOLD and total_skipped >= total_interactions / 2:\n",
        "        return \"low_engagement_user\"\n",
        "    else:\n",
        "        return \"active_user\"\n",
        "\n",
        "user_profiles = {classify_user_engagement(user_id): user_id for user_id in user_data['user_id'].unique()}\n",
        "\n",
        "# Generate recommendations\n",
        "def get_recommendations(user_id, n_recommendations=5, svd_weight=SVD_WEIGHT, lightfm_weight=LIGHTFM_WEIGHT):\n",
        "    combined_scores = {}\n",
        "    unwatched_videos = video_catalog[~video_catalog['video_link'].isin(user_data[user_data['user_id'] == user_id]['video_link'])]\n",
        "\n",
        "    for video in unwatched_videos['video_link']:\n",
        "        try:\n",
        "            est_rating = svd_model.predict(user_id, video).est\n",
        "            combined_scores[video] = combined_scores.get(video, 0) + est_rating * svd_weight\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error predicting rating for user {user_id} and video {video}: {e}\")\n",
        "\n",
        "    user_index = lfm_dataset.mapping()[0].get(str(user_id))\n",
        "    if user_index is not None:\n",
        "        scores = lfm_model.predict(user_index, np.arange(len(video_catalog)), item_features=item_features)\n",
        "        for i, score in enumerate(scores):\n",
        "            video = video_catalog.iloc[i]['video_link']\n",
        "            combined_scores[video] = combined_scores.get(video, 0) + score * lightfm_weight\n",
        "\n",
        "    recommendations = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:n_recommendations]\n",
        "    return [rec[0] for rec in recommendations]\n",
        "\n",
        "unique_user_ids = user_data['user_id'].unique().tolist()\n",
        "user_input = input(f\"Enter a user ID from the following options: {', '.join(unique_user_ids)} for recommendations: \")\n",
        "\n",
        "if user_input in unique_user_ids:\n",
        "    recommendations = get_recommendations(user_input, n_recommendations=5)\n",
        "    print(f\"Recommendations for user {user_input}:\", recommendations)\n",
        "else:\n",
        "    print(\"Invalid user ID. Please enter a valid user ID from the list:\", unique_user_ids)\n",
        "\n"
      ],
      "metadata": {
        "id": "cTOjfBFZkaAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install both 'surprise' and 'lightfm' libraries in Google Colab\n",
        "!pip install scikit-surprise lightfm\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from surprise import Dataset, Reader, SVD, accuracy\n",
        "from surprise.model_selection import train_test_split\n",
        "from lightfm import LightFM\n",
        "from lightfm.data import Dataset as LightFMDataset\n",
        "from lightfm.evaluation import precision_at_k\n",
        "import numpy as np\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "# Configure logging for errors only\n",
        "logging.basicConfig(filename=\"recommendation_system.log\", level=logging.ERROR)\n"
      ],
      "metadata": {
        "id": "Kz4wmAtRY1SY",
        "outputId": "692e80a2-af67-4e51-da40-232e6cf2f280",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m153.6/154.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lightfm\n",
            "  Downloading lightfm-1.17.tar.gz (316 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.4/316.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from lightfm) (2.32.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from lightfm) (1.5.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->lightfm) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->lightfm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->lightfm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->lightfm) (2024.8.30)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lightfm) (3.5.0)\n",
            "Building wheels for collected packages: scikit-surprise, lightfm\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp310-cp310-linux_x86_64.whl size=2357279 sha256=9cbd7f97c74376d181c3f1293277dc287fa85f7df339ed5d9f12c654ad70556b\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/3f/df/6acbf0a40397d9bf3ff97f582cc22fb9ce66adde75bc71fd54\n",
            "  Building wheel for lightfm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lightfm: filename=lightfm-1.17-cp310-cp310-linux_x86_64.whl size=806104 sha256=1a6fbeeba7aec958eb0740a878cbf57ab547c68acb68ec2d3b5c2997a21ad3aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/9b/7e/0b256f2168511d8fa4dae4fae0200fdbd729eb424a912ad636\n",
            "Successfully built scikit-surprise lightfm\n",
            "Installing collected packages: scikit-surprise, lightfm\n",
            "Successfully installed lightfm-1.17 scikit-surprise-1.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define URL parameters for flexibility\n",
        "USER_DATA_URL = \"https://raw.githubusercontent.com/javeriaz15/Video-Recommendation/refs/heads/main/dataset/RS_Fakedata-7-35_users.json\"\n",
        "VIDEO_CATALOG_URL = \"https://raw.githubusercontent.com/javeriaz15/Video-Recommendation/refs/heads/main/dataset/Video_catalog.json\"\n",
        "\n",
        "# Load data with error handling and retry mechanism\n",
        "def load_data(url):\n",
        "    try:\n",
        "        logging.info(f\"Loading data from {url}.\")\n",
        "        return pd.read_json(url)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading data from {url}: {e}\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if loading fails\n",
        "\n",
        "user_data = load_data(USER_DATA_URL)\n",
        "video_catalog = load_data(VIDEO_CATALOG_URL)\n",
        "user_data.head(), video_catalog.head()  # Check the first few rows to verify loading\n"
      ],
      "metadata": {
        "id": "fnJIxMseZzFK",
        "outputId": "c00d6715-3879-42b9-acd9-38b1493cb783",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(   user_id country         city state  age  \\\n",
              " 0        1     USA  Los Angeles    CA   30   \n",
              " 1        2     USA     New York    NY   30   \n",
              " 2        3     USA  Los Angeles    CA   18   \n",
              " 3        4  Canada      Toronto    ON   40   \n",
              " 4        5  Canada    Vancouver    BC   18   \n",
              " \n",
              "                                     video_link      genre  watched  liked  \\\n",
              " 0  https://www.youtube.com/watch?v=D4vN_5MBEog    hip hop      0.3  False   \n",
              " 1  https://www.youtube.com/watch?v=7iqMNnzQPmY     ballet      0.5  False   \n",
              " 2  https://www.youtube.com/watch?v=D4vN_5MBEog    hip hop      0.3   True   \n",
              " 3  https://www.youtube.com/watch?v=p0VGHuaICyI  classical      0.1  False   \n",
              " 4   https://www.youtube.com/shorts/fv5vCREiBMQ      k pop      0.1  False   \n",
              " \n",
              "    skipped           timestamp  \n",
              " 0     True 2024-10-30 08:00:00  \n",
              " 1     True 2024-10-30 08:02:00  \n",
              " 2     True 2024-10-30 08:04:00  \n",
              " 3     True 2024-10-30 08:06:00  \n",
              " 4     True 2024-10-30 08:08:00  ,\n",
              "    video_id                                   video_link      genre  country  \\\n",
              " 0         1  https://www.youtube.com/watch?v=D4vN_5MBEog    hip hop      USA   \n",
              " 1         2  https://www.youtube.com/watch?v=7iqMNnzQPmY     ballet      USA   \n",
              " 2         3  https://www.youtube.com/watch?v=p0VGHuaICyI  classical   Canada   \n",
              " 3         4   https://www.youtube.com/shorts/fv5vCREiBMQ      k pop   Canada   \n",
              " 4         5   https://www.youtube.com/shorts/kF0MRowRcIM    African  Nigeria   \n",
              " \n",
              "           city age_group  \n",
              " 0  Los Angeles     18-35  \n",
              " 1     New York     18-35  \n",
              " 2      Toronto     35-50  \n",
              " 3    Vancouver     18-25  \n",
              " 4         Kano     35-50  )"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Set up the collaborative filtering model with Surprise (SVD-based)\n",
        "reader = Reader(rating_scale=(0, 1))\n",
        "surprise_data = Dataset.load_from_df(user_data[['user_id', 'video_link', 'liked']], reader)\n",
        "trainset, testset = train_test_split(surprise_data, test_size=0.2)\n",
        "\n",
        "# Train the SVD model\n",
        "svd_model = SVD()\n",
        "svd_model.fit(trainset)\n",
        "\n",
        "# Predict on the test set and calculate RMSE\n",
        "predictions = svd_model.test(testset)\n",
        "rmse = accuracy.rmse(predictions)\n",
        "logging.info(f\"SVD Model RMSE: {rmse}\")\n",
        "print(f\"SVD Model RMSE: {rmse}\")\n",
        "\n",
        "# rating are binary: 0 for \"not liked\" and 1 for \"liked\".\n",
        "# an RMSE closer to 0 (like 0.4764 here) still indicates that the model is effectively predicting ratings in line with actual user preferences\n",
        "# can be small but not; dataset is small or lacks diversity in user interactions\n",
        "'''"
      ],
      "metadata": {
        "id": "LEOK1CCrcZ2b",
        "outputId": "641cf9b9-996d-439c-bf2b-47d0a1aa320f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 0.4764\n",
            "SVD Model RMSE: 0.47639078699546056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# option 2 of above\n",
        "# Set up the collaborative filtering model with Surprise (SVD-based)\n",
        "from surprise import Dataset, Reader, SVD, accuracy\n",
        "from surprise.model_selection import train_test_split, cross_validate, GridSearchCV\n",
        "\n",
        "reader = Reader(rating_scale=(0, 1))\n",
        "surprise_data = Dataset.load_from_df(user_data[['user_id', 'video_link', 'liked']], reader)\n",
        "\n",
        "# Define the parameter grid for GridSearch\n",
        "param_grid = {\n",
        "    'n_factors': [100, 200, 250, 300],\n",
        "    'n_epochs': [20, 30, 50],\n",
        "    'lr_all': [0.002, 0.003, 0.005],\n",
        "    'reg_all': [0.15, 0.18, 0.2]  # Increased regularization values\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV to find the best hyperparameters\n",
        "grid_search = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
        "grid_search.fit(surprise_data)\n",
        "\n",
        "# Retrieve the best parameters and RMSE score from the grid search\n",
        "best_params = grid_search.best_params['rmse']\n",
        "best_rmse = grid_search.best_score['rmse']\n",
        "logging.info(f\"Best RMSE Score from Grid Search: {best_rmse}\")\n",
        "logging.info(f\"Best Hyperparameters: {best_params}\")\n",
        "print(f\"Best RMSE Score from Grid Search: {best_rmse}\")\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n",
        "\n",
        "# Train the optimized SVD model using cross-validation\n",
        "logging.info(\"Evaluating the optimized model with cross-validation.\")\n",
        "final_model = SVD(**best_params)\n",
        "cross_val_results = cross_validate(final_model, surprise_data, measures=['rmse'], cv=10, verbose=True)\n",
        "average_rmse = cross_val_results['test_rmse'].mean()\n",
        "print(f\"Average RMSE with Cross-Validation: {average_rmse}\")\n",
        "logging.info(f\"Average RMSE with Cross-Validation: {average_rmse}\")\n",
        "\n",
        "# Optional: If final evaluation on a test set is desired\n",
        "trainset, testset = train_test_split(surprise_data, test_size=0.2)\n",
        "final_model.fit(trainset)\n",
        "predictions = final_model.test(testset)\n",
        "test_rmse = accuracy.rmse(predictions)\n",
        "logging.info(f\"Optimized SVD Model Test RMSE: {test_rmse}\")\n",
        "print(f\"Optimized SVD Model Test RMSE: {test_rmse}\")"
      ],
      "metadata": {
        "id": "WwTuf3oGd_1B",
        "outputId": "e68eae98-5df5-4915-ab62-ca03a32a81ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best RMSE Score from Grid Search: 0.4627438780679937\n",
            "Best Hyperparameters: {'n_factors': 250, 'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.18}\n",
            "Evaluating RMSE of algorithm SVD on 10 split(s).\n",
            "\n",
            "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Fold 6  Fold 7  Fold 8  Fold 9  Fold 10 Mean    Std     \n",
            "RMSE (testset)    0.6108  0.4794  0.4164  0.5463  0.6602  0.4738  0.3217  0.2713  0.4744  0.5490  0.4803  0.1147  \n",
            "Fit time          0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    \n",
            "Test time         0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    \n",
            "Average RMSE with Cross-Validation: 0.48033054904706923\n",
            "RMSE: 0.5141\n",
            "Optimized SVD Model Test RMSE: 0.5140954270425315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure unique video_link values in video_catalog before setting index\n",
        "video_catalog = video_catalog.drop_duplicates(subset=['video_link'])\n",
        "\n",
        "# Encode video features for LightFM model\n",
        "video_catalog['video_id'] = video_catalog['video_link'].factorize()[0]\n",
        "user_data['user_id'] = user_data['user_id'].astype(str)\n",
        "user_data['video_id'] = user_data['video_link'].map(video_catalog.set_index('video_link')['video_id'])\n",
        "\n",
        "# Initialize LightFM dataset with all item feature types\n",
        "lfm_dataset = LightFMDataset()\n",
        "lfm_dataset.fit(\n",
        "    users=(x for x in user_data['user_id'].unique()),\n",
        "    items=(x for x in video_catalog['video_id'].unique()),\n",
        "    item_features=(x for x in pd.concat([video_catalog['genre'], video_catalog['age_group'],\n",
        "                                         video_catalog['city'], video_catalog['country']]).unique())\n",
        ")\n",
        "\n",
        "# Define feature extraction function and generate item features\n",
        "def extract_item_features(df, feature_cols):\n",
        "    return list(zip(df['video_id'], df[feature_cols].apply(lambda x: list(map(str, x)), axis=1)))\n",
        "\n",
        "# Define the feature columns based on the actual data structure in video_catalog\n",
        "feature_columns = ['genre', 'age_group', 'city', 'country']\n",
        "item_features = lfm_dataset.build_item_features(extract_item_features(video_catalog, feature_columns))\n",
        "\n",
        "# Define the range of parameters to test for LightFM - BASELINE\n",
        "loss_functions = ['warp', 'bpr', 'warp-kos']\n",
        "epochs = [15, 30] # adjust epochs later\n",
        "learning_rates = [0.01, 0.05]\n",
        "embedding_sizes = [30, 50]  # no_components # adjust epochs later\n",
        "\n",
        "# Initialize variables to store best results\n",
        "best_precision = 0\n",
        "best_params = {}\n",
        "\n",
        "# Loop through each combination of parameters\n",
        "for loss in loss_functions:\n",
        "    for epoch in epochs:\n",
        "        for lr in learning_rates:\n",
        "            for n_components in embedding_sizes:\n",
        "                # Train LightFM model with the current set of parameters\n",
        "                lfm_model = LightFM(loss=loss, learning_rate=lr, no_components=n_components)\n",
        "                (interactions, weights) = lfm_dataset.build_interactions(\n",
        "                    ((str(row['user_id']), row['video_id']) for _, row in user_data.iterrows())\n",
        "                )\n",
        "                lfm_model.fit(interactions, item_features=item_features, epochs=epoch, num_threads=4)\n",
        "\n",
        "                # Calculate Precision@5 for LightFM\n",
        "                precision = precision_at_k(lfm_model, interactions, item_features=item_features, k=5).mean()\n",
        "                logging.info(f\"Loss Function: {loss}, Epochs: {epoch}, Learning Rate: {lr}, Components: {n_components}, Precision: {precision}\")\n",
        "                print(f\"Loss Function: {loss}, Epochs: {epoch}, Learning Rate: {lr}, Components: {n_components}, Precision: {precision}\")\n",
        "\n",
        "                # Update best precision and parameters if current precision is higher\n",
        "                if precision > best_precision:\n",
        "                    best_precision = precision\n",
        "                    best_params = {'loss': loss, 'epochs': epoch, 'learning_rate': lr, 'no_components': n_components}\n",
        "\n",
        "# Display the best parameters and corresponding precision score\n",
        "print(f\"\\nBest Precision: {best_precision} with parameters: {best_params}\")\n",
        "logging.info(f\"Best Precision: {best_precision} with parameters: {best_params}\")\n",
        "\n",
        "\n",
        "'''\n",
        "Result: Best Precision: 0.8285714387893677 with parameters: {'loss': 'warp-kos', 'epochs': 30, 'learning_rate': 0.05, 'no_components': 30}\n",
        "0.83 means that in the top 5 recommendations, about 83% of the items are relevant\n",
        "'''"
      ],
      "metadata": {
        "id": "OBOlp-i8cj4K",
        "outputId": "7d212524-e658-4826-bfc7-afcdb3daf13e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss Function: warp, Epochs: 15, Learning Rate: 0.01, Components: 30, Precision: 0.3142857253551483\n",
            "Loss Function: warp, Epochs: 15, Learning Rate: 0.01, Components: 50, Precision: 0.3142857253551483\n",
            "Loss Function: warp, Epochs: 15, Learning Rate: 0.05, Components: 30, Precision: 0.5428571701049805\n",
            "Loss Function: warp, Epochs: 15, Learning Rate: 0.05, Components: 50, Precision: 0.5142857432365417\n",
            "Loss Function: warp, Epochs: 30, Learning Rate: 0.01, Components: 30, Precision: 0.3142857253551483\n",
            "Loss Function: warp, Epochs: 30, Learning Rate: 0.01, Components: 50, Precision: 0.3142857253551483\n",
            "Loss Function: warp, Epochs: 30, Learning Rate: 0.05, Components: 30, Precision: 0.7142857313156128\n",
            "Loss Function: warp, Epochs: 30, Learning Rate: 0.05, Components: 50, Precision: 0.6571429371833801\n",
            "Loss Function: bpr, Epochs: 15, Learning Rate: 0.01, Components: 30, Precision: 0.2857142984867096\n",
            "Loss Function: bpr, Epochs: 15, Learning Rate: 0.01, Components: 50, Precision: 0.2857142984867096\n",
            "Loss Function: bpr, Epochs: 15, Learning Rate: 0.05, Components: 30, Precision: 0.3142857253551483\n",
            "Loss Function: bpr, Epochs: 15, Learning Rate: 0.05, Components: 50, Precision: 0.3142857551574707\n",
            "Loss Function: bpr, Epochs: 30, Learning Rate: 0.01, Components: 30, Precision: 0.2857142984867096\n",
            "Loss Function: bpr, Epochs: 30, Learning Rate: 0.01, Components: 50, Precision: 0.2857142984867096\n",
            "Loss Function: bpr, Epochs: 30, Learning Rate: 0.05, Components: 30, Precision: 0.2857142984867096\n",
            "Loss Function: bpr, Epochs: 30, Learning Rate: 0.05, Components: 50, Precision: 0.3142857253551483\n",
            "Loss Function: warp-kos, Epochs: 15, Learning Rate: 0.01, Components: 30, Precision: 0.34285715222358704\n",
            "Loss Function: warp-kos, Epochs: 15, Learning Rate: 0.01, Components: 50, Precision: 0.40000003576278687\n",
            "Loss Function: warp-kos, Epochs: 15, Learning Rate: 0.05, Components: 30, Precision: 0.6285714507102966\n",
            "Loss Function: warp-kos, Epochs: 15, Learning Rate: 0.05, Components: 50, Precision: 0.5428571105003357\n",
            "Loss Function: warp-kos, Epochs: 30, Learning Rate: 0.01, Components: 30, Precision: 0.5999999642372131\n",
            "Loss Function: warp-kos, Epochs: 30, Learning Rate: 0.01, Components: 50, Precision: 0.5428571105003357\n",
            "Loss Function: warp-kos, Epochs: 30, Learning Rate: 0.05, Components: 30, Precision: 0.8285714387893677\n",
            "Loss Function: warp-kos, Epochs: 30, Learning Rate: 0.05, Components: 50, Precision: 0.800000011920929\n",
            "\n",
            "Best Precision: 0.8285714387893677 with parameters: {'loss': 'warp-kos', 'epochs': 30, 'learning_rate': 0.05, 'no_components': 30}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nResult: Best Precision: 0.8285714387893677 with parameters: {'loss': 'warp-kos', 'epochs': 30}\\n0.83 means that in the top 5 recommendations, about 83% of the items are relevant\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Time decay function to adjust ratings based on recency of interaction\n",
        "def time_decay(timestamp, decay_rate=0.001):\n",
        "    days_ago = (datetime.now() - datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%S\")).days\n",
        "    return np.exp(-decay_rate * days_ago)\n",
        "\n",
        "# Apply time decay to liked ratings\n",
        "user_data['decayed_liked'] = user_data.apply(\n",
        "    lambda row: row['liked'] * time_decay(row['timestamp']) if row['liked'] else 0, axis=1\n",
        ")\n",
        "\n",
        "# Calculate engagement thresholds based on data\n",
        "MIN_INTERACTIONS_FOR_ACTIVE = int(user_data.groupby('user_id').size().quantile(0.5))\n",
        "HIGH_WATCH_THRESHOLD = user_data['watched'].quantile(0.75)\n",
        "LOW_WATCH_THRESHOLD = user_data['watched'].quantile(0.25)\n",
        "\n",
        "print(\"Engagement thresholds set:\", MIN_INTERACTIONS_FOR_ACTIVE, HIGH_WATCH_THRESHOLD, LOW_WATCH_THRESHOLD)\n"
      ],
      "metadata": {
        "id": "ixhp5wQ8ckl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache user interaction metrics for engagement classification\n",
        "user_interactions_cache = user_data.groupby('user_id').apply(lambda x: {\n",
        "    'total_interactions': len(x),\n",
        "    'avg_watch': x['watched'].mean(),\n",
        "    'total_skipped': x['skipped'].sum()\n",
        "}).to_dict()\n",
        "\n",
        "# Classify engagement\n",
        "def classify_user_engagement(user_id):\n",
        "    user_metrics = user_interactions_cache.get(user_id, {})\n",
        "    total_interactions = user_metrics.get('total_interactions', 0)\n",
        "    avg_watch = user_metrics.get('avg_watch', 0)\n",
        "    total_skipped = user_metrics.get('total_skipped', 0)\n",
        "\n",
        "    if total_interactions < MIN_INTERACTIONS_FOR_ACTIVE:\n",
        "        return \"new_user\"\n",
        "    elif avg_watch < LOW_WATCH_THRESHOLD and total_skipped >= total_interactions / 2:\n",
        "        return \"low_engagement_user\"\n",
        "    else:\n",
        "        return \"active_user\"\n",
        "\n",
        "# Assign profiles to users\n",
        "user_profiles = {classify_user_engagement(user_id): user_id for user_id in user_data['user_id'].unique()}\n",
        "print(\"User profiles assigned:\", user_profiles)\n"
      ],
      "metadata": {
        "id": "AM-Ow0JDcmv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate blended recommendations\n",
        "def get_recommendations(user_id, n_recommendations=5, svd_weight=SVD_WEIGHT, lightfm_weight=LIGHTFM_WEIGHT):\n",
        "    combined_scores = {}\n",
        "    unwatched_videos = video_catalog[~video_catalog['video_link'].isin(user_data[user_data['user_id'] == user_id]['video_link'])]\n",
        "\n",
        "    for video in unwatched_videos['video_link']:\n",
        "        try:\n",
        "            est_rating = svd_model.predict(user_id, video).est\n",
        "            combined_scores[video] = combined_scores.get(video, 0) + est_rating * svd_weight\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error predicting rating for user {user_id} and video {video}: {e}\")\n",
        "\n",
        "    user_index = lfm_dataset.mapping()[0].get(str(user_id))\n",
        "    if user_index is not None:\n",
        "        scores = lfm_model.predict(user_index, np.arange(len(video_catalog)), item_features=item_features)\n",
        "        for i, score in enumerate(scores):\n",
        "            video = video_catalog.iloc[i]['video_link']\n",
        "            combined_scores[video] = combined_scores.get(video, 0) + score * lightfm_weight\n",
        "\n",
        "    recommendations = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:n_recommendations]\n",
        "    return [rec[0] for rec in recommendations]\n",
        "\n",
        "# Test recommendations by input\n",
        "unique_user_ids = user_data['user_id'].unique().tolist()\n",
        "user_input = input(f\"Enter a user ID from the following options: {', '.join(unique_user_ids)} for recommendations: \")\n",
        "\n",
        "if user_input in unique_user_ids:\n",
        "    recommendations = get_recommendations(user_input, n_recommendations=5)\n",
        "    print(f\"Recommendations for user {user_input}:\", recommendations)\n",
        "else:\n",
        "    print(\"Invalid user ID. Please enter a valid user ID from the list:\", unique_user_ids)\n"
      ],
      "metadata": {
        "id": "DgR_affkcqdF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}